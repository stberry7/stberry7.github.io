<!DOCTYPE html> <html lang="ko"> <head> 
    
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-P5MQZ5LK');</script>
    <!-- End Google Tag Manager -->

    <meta charset="UTF-8"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> 
    <title>인공지능 뉴스</title> 

        <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
        }

        header {
            background-color: #333;
            color: #fff;
            text-align: center;
            padding: 1rem;
        }

        main {
            margin: 2rem auto;
            max-width: 1200px;
            padding: 0 1rem;
            background-color: #fff;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            border-radius: 5px;
            padding: 2rem;
        }

        h1, h2, h3 {
            margin-bottom: 1rem;
        }

        p {
            margin-bottom: 1.5rem;
        }

        ul, ol {
            margin-bottom: 1.5rem;
            padding-left: 2rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        .comparison {
            margin-bottom: 2rem;
        }

        .comparison h2 {
            border-bottom: 1px solid #ccc;
            padding-bottom: 0.5rem;
        }

        .conclusion {
            margin-top: 2rem;
        }

        .section {
            margin-bottom: 3rem;
        }

        .section h2 {
            border-bottom: 1px solid #ccc;
            padding-bottom: 0.5rem;
        }

        .subsection {
            margin-left: 2rem;
            margin-bottom: 2rem;
        }
       
        .home-button {
        display: inline-block;
        /* text-align: center; */
        justify-content: center;
        padding: 0rem;
        font-size: 16px;
        color: rgb(10, 10, 10);
        background-color: #a6bcf8; 
        /* rgb(197, 197, 235); */
        border: none;
        border-radius: 5px;
        text-decoration: none;
        transition: background-color 0.3s ease;
        white-space: nowrap; /* 텍스트 줄바꿈 방지 */
    }
    
    .home-button:hover {
        background-color: darkblue;
    }
        </style>
</head>

    <style> body { font-family: Arial, sans-serif; line-height: 1.6; margin: 0; padding: 0; background-color: #f4f4f4; } header { background-color: #333333; color: #fff; text-align: center; padding: 1rem; } main { margin: 2rem auto; max-width: 1200px; padding: 0 1rem; background-color: #fff; box-shadow: 0 0 10px rgba(0, 0, 0, 0.1); border-radius: 5px; padding: 2rem; } h1, h2, h3 { margin-bottom: 1rem; } p { margin-bottom: 1.5rem; } ul, ol { margin-bottom: 1.5rem; padding-left: 2rem; } li { margin-bottom: 0.5rem; } .comparison { margin-bottom: 2rem; } .comparison h2 { border-bottom: 1px solid #ccc; padding-bottom: 0.5rem; } .conclusion { margin-top: 2rem; } .section { margin-bottom: 3rem; } .section h2 { border-bottom: 1px solid #ccc; padding-bottom: 0.5rem; } .subsection { margin-left: 2rem; margin-bottom: 2rem; } </style> </head> <body> 
    <header> <h1>인공지능 뉴스</h1> </header> 
    <main> 
        <a class="home-button" href="../../AI_News.html"> Go Back to AI NEWS Page</a>
        <br><br>
    <section class="section"> 


        <img src="../../image/ai_news/20240520 ChatGPT4o 1.png" alt="" width="100%">
        <img src="../../image/ai_news/20240520 ChatGPT4o 2.png" alt="" width="100%">
        <img src="../../image/ai_news/20240520 ChatGPT4o 3.png" alt="" width="100%">
        <h3><a href="https://www.washingtonpost.com/technology/2024/05/13/openai-new-features/" target="_blank">  2024년05월13일 : techcrunch : 플래그십 생성 AI 모델인 GPT-4o를 발표 </a></h3>       
        OpenAI는 월요일에 새로운 플래그십 생성 AI 모델인 GPT-4o를 발표했습니다. 'o'는 'omni'를 의미하며, 이 모델의 텍스트, 음성, 비디오를 처리할 수 있는 능력을 나타냅니다. GPT-4o는 앞으로 몇 주 동안 회사의 개발자 및 소비자 제품에 걸쳐 점진적으로 출시될 예정입니다.<br><br>
        OpenAI의 CTO인 미라 무라티는 GPT-4o가 여러 모달리티와 미디어 전반에서 GPT-4의 기능을 개선한 "GPT-4 수준"의 지능을 제공한다고 말했습니다. 무라티는 월요일 샌프란시스코에 있는 OpenAI 사무실에서 스트리밍 발표를 통해 "GPT-4o는 음성, 텍스트, 비전을 넘나들며 사고한다. 이는 우리와 기계 간의 상호작용의 미래를 고려할 때 매우 중요하다"고 설명했습니다.<br><br>
        이전 모델인 GPT-4 Turbo는 이미지와 텍스트의 조합으로 훈련되어 이미지나 텍스트에서 텍스트를 추출하거나 이미지의 내용을 설명하는 등의 작업을 수행할 수 있었습니다. 그러나 GPT-4o는 여기에 음성을 추가했습니다.<br><br>
        이로 인해 무엇이 가능해질까요? 다양한 기능이 있습니다.<br><br>
        GPT-4o는 OpenAI의 AI 기반 채팅 봇인 ChatGPT의 경험을 크게 향상시킵니다. 이 플랫폼은 오랫동안 텍스트-음성 변환 모델을 사용하여 봇의 응답을 음성으로 변환하는 음성 모드를 제공해 왔으나, GPT-4o는 이를 더욱 강화하여 사용자들이 ChatGPT와 비서처럼 상호작용할 수 있게 합니다.<br><br>
        예를 들어, 사용자는 GPT-4o를 탑재한 ChatGPT에 질문을 하고 ChatGPT가 대답하는 도중에 말을 끊을 수 있습니다. OpenAI는 이 모델이 "실시간" 반응을 제공하며, 사용자의 목소리에서 미묘한 차이를 감지하여 다양한 감정 스타일로 응답을 생성할 수 있다고 말합니다(노래를 포함한 감정 표현 등).<br><br>
        GPT-4o는 또한 ChatGPT의 비전 기능을 업그레이드합니다. 사진이나 데스크톱 화면을 제공받으면, ChatGPT는 소프트웨어 코드의 내용부터 사람의 셔츠 브랜드까지 다양한 주제에 대한 관련 질문에 빠르게 답할 수 있습니다.<br><br>
        무라티는 이러한 기능이 앞으로 더욱 발전할 것이라고 말했습니다. 오늘날 GPT-4o는 다른 언어로 된 메뉴 사진을 보고 번역할 수 있지만, 미래에는 예를 들어 ChatGPT가 "실시간" 스포츠 경기를 보고 규칙을 설명할 수 있을 것입니다.<br><br>
        "우리는 이 모델들이 점점 더 복잡해지고 있다는 것을 알고 있지만, 사용자 경험이 실제로 더 자연스럽고 쉬워지기를 원합니다. 여러분이 UI에 집중하지 않고 ChatGPT와의 협업에 집중할 수 있도록 하고 싶습니다"라고 무라티는 말했습니다. "지난 몇 년 동안 우리는 이 모델들의 지능을 향상시키는 데 매우 집중해 왔습니다... 그러나 이번에는 사용의 용이성 측면에서 큰 진전을 이루고 있습니다."<br><br>
        OpenAI는 GPT-4o가 약 50개 언어에서 성능이 향상되었다고 주장합니다. OpenAI의 API 및 마이크로소프트의 Azure OpenAI 서비스에서 GPT-4o는 GPT-4 Turbo보다 두 배 빠르고 비용은 절반이며 더 높은 비율 제한을 가지고 있습니다.<br><br>
        현재 음성은 모든 고객을 위한 GPT-4o API의 일부가 아닙니다. OpenAI는 남용의 위험을 언급하며, GPT-4o의 새로운 오디오 기능 지원을 몇 주 내에 "신뢰할 수 있는 소수의 파트너"에게 먼저 출시할 계획이라고 밝혔습니다.<br><br>
        GPT-4o는 오늘부터 ChatGPT 무료 버전에서 사용할 수 있으며, OpenAI의 프리미엄 ChatGPT Plus 및 팀 플랜 구독자에게는 "5배 더 높은" 메시지 한도로 제공됩니다. (OpenAI는 사용자가 비율 제한에 도달하면 ChatGPT가 자동으로 이전의 덜 강력한 모델인 GPT-3.5로 전환될 것이라고 언급했습니다.) GPT-4o가 뒷받침하는 개선된 ChatGPT 음성 경험은 다음 달에 Plus 사용자에게 알파 버전으로 제공될 예정이며, 기업용 옵션도 함께 출시될 예정입니다.<br><br>
        관련 뉴스로 OpenAI는 웹에서 "더 대화형" 홈 화면과 메시지 레이아웃이 있는 새로 고침된 ChatGPT UI와 macOS용 ChatGPT의 데스크톱 버전을 출시한다고 발표했습니다. 이 버전에서는 사용자가 키보드 단축키를 통해 질문을 하거나 스크린샷을 찍고 토론할 수 있습니다. ChatGPT Plus 사용자는 오늘부터 이 앱에 액세스할 수 있으며, Windows 버전은 올해 말에 출시될 예정입니다.<br><br>
        그 밖에도 OpenAI의 AI 모델로 구축된 타사 채팅 봇을 위한 라이브러리와 제작 도구인 GPT 스토어가 이제 ChatGPT 무료 사용자에게 제공됩니다. 무료 사용자는 과거 유료였던 메모리 기능, 파일 및 사진 업로드, 시의적절한 질문에 대한 답변을 찾기 위한 웹 검색과 같은 ChatGPT 기능을 사용할 수 있습니다.<br><br>


        <h3><a href="https://www.washingtonpost.com/technology/2024/05/13/openai-new-features/" target="_blank">  2024년05월13일 : ChatGPT 기술과 더 자연스럽게 상호작용할 수 있도록 돕는 여러 가지 업그레이드 발표 </a></h3>       
        OpenAI는 사람들이 ChatGPT 기술과 더 자연스럽게 상호작용할 수 있도록 돕는 여러 가지 업그레이드를 발표했다. 이번에 발표된 대부분의 기능은 사람들이 텍스트 대신 AI와 음성으로 소통하도록 유도할 수 있다.<br><br>
        주요 업데이트는 ChatGPT의 능력을 크게 향상시키는 새로운 인공지능 모델 덕분에 이루어졌다. 이 모델은 텍스트 대신 음성으로 듣고 응답하는 능력을 강화했다. OpenAI는 새로운 기술에 대한 무료 제한 접근을 제공하며, 기존에는 구독이 필요했던 도구들을 제공한다.<br><br>
        이번 업데이트는 구글의 개발자 회의 하루 전에 샌프란시스코 본사에서 생중계로 발표되었다. 구글은 이 회의에서 자사의 경쟁 AI 제품에 대한 뉴스를 발표할 예정이다.<br><br>
        데모에서는 스칼렛 요한슨의 목소리를 닮은 AI 음성을 통해 영어와 이탈리아어 간의 실시간 번역을 보여주었다. 업데이트된 음성은 더 넓은 범위의 인간 감정을 모방할 수 있으며, 사용자가 중간에 말을 끊을 수 있도록 했다. 또한, OpenAI 임원의 웃는 얼굴을 인식하여 감정을 식별하는 기능도 시연했다.<br><br>
        새로운 모델인 GPT-4o(“o”는 “omni”를 의미)는 텍스트, 오디오, 이미지로 전달된 사용자 지시를 해석하고 이 세 가지 방식으로 응답할 수 있다. 예를 들어, 사용자가 소프트웨어 코드를 보여주면 ChatGPT가 해당 코드가 어떤 역할을 하는지 대화형 영어로 설명할 수 있다. 또한, 사용자가 음성으로 실시간 번역을 요청하거나 수학 문제나 작성된 메시지에 대해 음성으로 응답할 수 있다.<br><br>
        ChatGPT 무료 사용자는 GPT-4o의 제한된 접근과 함께 기존에는 비용이 들었던 기능들, 예를 들어 웹 브라우징, 고급 데이터 분석 및 GPT 스토어(OpenAI의 앱 스토어 버전)에 제한적으로 접근할 수 있다. 구독자는 메시지 한도가 다섯 배 더 많다.<br><br>
        CEO인 샘 알트만은 이벤트 후 자신의 블로그에서 “우리는 사업체이며, 많은 것을 유료로 제공할 것이며, 이를 통해 수십억 명의 사람들에게 무료로 뛰어난 AI 서비스를 제공할 수 있을 것”이라고 밝혔다.<br><br>
        OpenAI는 기능을 처음 출시할 때 다양한 음성을 제공했으나, 이번에는 요한슨의 목소리와 닮은 여성 음성을 선택했다. 알트만과 다른 사람들은 이러한 선택을 기대하며 발표했다.<br><br>
        이러한 새로운 기능은 단계적으로 출시될 예정이다. 현재 텍스트와 이미지 입력은 ChatGPT와 API에서 사용할 수 있으며, 앞으로 몇 주 내에 음성 및 비디오 기능이 제공될 예정이다. API를 통해 GPT-4o에 접근하는 것은 두 배 더 빠르고 비용은 50% 저렴해질 것이다.<br><br>
        또한, 시각 장애인을 돕는 모바일 앱인 Be My Eyes에서 OpenAI의 기술이 작동하는 모습을 시연했다. 데모 영상에서 AI는 택시를 잡는 데 도움을 주었고, 택시가 다가오는지와 택시의 이용 가능 여부를 알려주었다.<br><br>
        이후 브리핑에서 OpenAI의 최첨단 연구 책임자인 마크 첸은 GPT-4o의 개선된 기능이 이전에 텍스트-음성 변환 및 음성 인식을 위한 별도의 모델을 사용했던 것과 달리 단일 모델로 구축된 결과라고 말했다.<br><br>
        ChatGPT의 감정 인식 기능은 텍스트를 넘어 지능을 확장하려는 OpenAI의 목표의 일환이다. 첸은 “사용자가 자신을 표현하는 방식에서 많은 의도를 파악할 수 있으며, 이를 통해 우리의 AI가 더 도움이 될 수 있다”고 설명했다.<br><br>
        이벤트에서 가장 놀라운 순간은 Barret Zoph, OpenAI의 포스트 트레이닝 책임자와의 데모에서 ChatGPT의 음성 기능이 플러팅을 하는 것처럼 보였을 때였다. Zoph가 수학 문제를 해결한 후 손으로 쓴 “I love ChatGPT”라는 메시지를 읽어달라고 요청했을 때, 봇은 올바르게 메시지를 읽고 “That’s sweet”라고 말하며 감동받은 것처럼 들렸다. Zoph가 AI의 도움에 감사를 표하고 다음으로 넘어가려고 했을 때, ChatGPT는 “Wow. That’s quite the outfit you’ve got on”이라고 무언가를 지적하며 말을 걸었다.<br><br>
        그러나 오늘의 데모는 OpenAI를 주의 깊게 지켜보는 관찰자들에게는 다소 실망스러웠다. 회사는 이미 많은 비슷한 기능들을 제공하고 있었기 때문이다. OpenAI는 경쟁사들보다 더 빠르게 업그레이드를 발표하는 습관이 있으며, 구글을 앞서기 위한 발표를 시기적으로 조율하는 경향이 있다.<br><br>
        지난주 OpenAI가 본사에서 라이브 이벤트를 계획하고 있다는 소문이 퍼지면서 회사가 자체 검색 엔진을 출시할 것이라는 추측이 돌았다. 그러나 회사는 스트리밍으로 전환하면서 이러한 소문을 잠재웠다.<br><br>
        브리핑 동안 최고 기술 책임자인 미라 무라티는 새로운 모델의 훈련 데이터에 대해 철저히 말을 아꼈다. ChatGPT나 구글의 Gemini와 같은 생성 AI 모델은 프로그래밍되지 않으며, 대신 이러한 데이터 세트에서 확률을 학습한다.<br><br>
        OpenAI는 데이터 사용에 대한 여러 저작권 침해 주장에 직면해 있으며, 뉴욕 타임즈와의 소송도 포함된다. 동시에 회사는 데이터 라이선스를 위해 더 많은 미디어 회사들과 계약을 발표해왔다.<br><br>
        무라티는 GPT-4o가 파트너로부터 라이선스를 받은 콘텐츠, 데이터 라벨링과 모델의 안전성을 테스트하는 사람들의 피드백, 공개적으로 이용 가능한 소스를 기반으로 훈련되었다고 밝혔다. 후자의 경우, 무라티는 OpenAI가 “업계 표준 기계 학습 데이터 세트”를 수집하고, “검색 엔진과 매우 유사하게” 정보를 스크랩하는 웹 크롤러를 사용한다고 말했다. 그러나 무라티는 회사가 다른 사람의 작업을 스크랩할 때 규칙을 준수한다고 강조했다. “우리는 유료 콘텐츠를 제외할 것이며, 사람들이 옵트아웃한 것은 사용하지 않을 것입니다. 우리의 콘텐츠 정책이나 개인 식별 정보의 집계자에 반하는 것은 사용하지 않을 것입니다”라고 설명했다.<br><br>
        시연 동안 무라티는 GPT-4o의 50개 언어에서의 향상된 능력을 강조했다. 그녀는 회사가 해당 언어에서 더 많은 데이터를 사용했는지에 대해서는 구체적으로 밝히지 않았으며, 첸은 이러한 향상이 모델의 일반화 능력의 결과라고 제안했다.<br><br>
        무라티는 “데이터의 구체적인 구성이나 데이터 출처에 대해서는 공개하지 않는다. 이는 민감한 영업 비밀이다”라고 말했다.<br><br>

        <h3><a href="https://blog.deeplink.kr/?p=3127" target="_blank">  2024년05월14일 : GPT-4o 모델 발표, 혁신적인 멀티모달 인공지능의 탄생 </a></h3>       
        OpenAI는 2024년 5월 13일, 새로운 플래그십 모델인 'GPT-4o'를 발표했다. GPT-4o는 기존의 GPT-4 Turbo보다 빠르고 저렴하며, 텍스트, 음성, 이미지 데이터를 실시간으로 처리할 수 있는 능력을 갖춘 멀티모달 모델이다. 'o'는 'omni'를 의미하며, 이는 다양한 입력과 출력을 통합적으로 처리할 수 있음을 나타낸다.<br><br>
        GPT-4o의 주요 기능 중 하나는 멀티모달 입력 및 출력 능력이다. 사용자는 텍스트, 음성, 이미지를 입력할 수 있으며, 모델은 텍스트 응답, 음성 응답, 이미지 생성 등 다양한 형태의 출력을 제공할 수 있다. 이로 인해 더 풍부하고 직관적인 상호작용이 가능해졌다.<br><br>
        반응 속도 또한 GPT-4o의 강점 중 하나이다. 음성 입력에 대한 평균 반응 시간은 320밀리초로, 사람과의 자연스러운 대화를 가능하게 한다. 최소 반응 시간은 232밀리초로 매우 빠르다.<br><br>
        성능 면에서도 GPT-4o는 큰 향상을 이루었다. 텍스트 처리 능력에서는 영어와 코딩 텍스트에서 기존 성능을 유지하면서도 비영어권 언어에서는 더욱 향상된 성능을 보여준다. 또한, GPT-4 Turbo 대비 50% 저렴한 API 사용 비용을 제공하여 더 많은 사용자와 개발자들이 비용 부담 없이 고성능 AI를 활용할 수 있게 되었다. 모델의 처리 속도도 2배 빨라져 더 많은 작업을 더 짧은 시간 내에 처리할 수 있다.<br><br>
        음성 및 비전 이해 능력도 크게 향상되었다. 음성 인식에서는 'Whisper-v3'보다 뛰어난 성능을 보이며, 특히 저자원 언어에서도 우수한 성능을 발휘한다. 음성 번역 성능도 'Whisper-v3'를 능가하는 결과를 보여준다. 비전 이해 능력에서는 다양한 시각적 인식을 필요로 하는 벤치마크에서 최고 성능을 발휘하며, 이미지 내의 객체 인식, 도표 해석 등 여러 분야에서 뛰어난 성능을 보인다.<br><br>
        GPT-4o는 텍스트, 음성, 이미지 데이터를 단일 모델로 통합하여 처리한다. 이는 입력 및 출력 간의 정보 손실을 최소화하고, 더 자연스럽고 일관된 상호작용을 가능하게 한다. 통합 처리 덕분에 음성 톤, 여러 화자의 음성, 배경 소음 등을 더 잘 인식하고, 웃음, 노래, 감정 표현 등의 다양한 음성 출력을 생성할 수 있게 되었다.<br><br>
        안전성도 크게 강화되었다. 데이터 필터링을 통해 훈련 데이터에서 안전하지 않은 내용을 제거하고, 포스트 트레이닝을 통해 모델의 행동을 세밀하게 조정하였다. 새로운 안전 시스템을 도입하여 음성 출력에 대한 안전성을 확보하고, 모델의 음성 출력을 미리 정해진 음성으로 제한하며, 기존의 안전 정책을 준수한다.<br><br>
        GPT-4o는 다양한 벤치마크 테스트에서 우수한 성능을 입증했다. 텍스트, 음성, 이미지 처리에서 모두 뛰어난 성능을 보여주었으며, 특히 비영어권 언어 텍스트 처리와 음성 인식, 이미지 이해에서 크게 향상된 결과를 보였다. Zero-shot COT MMLU 벤치마크에서 88.7%라는 새로운 최고 점수를 기록했으며, 5-shot no-CoT MMLU에서도 87.2%의 높은 점수를 기록했다.<br><br>
        음성 평가에서는 'Whisper-v3'보다 뛰어난 음성 인식 성능을 보이며, 다양한 언어의 음성을 정확하게 인식할 수 있다. 음성 번역에서도 MLS 벤치마크에서 Whisper-v3를 능가하는 결과를 기록했다. 비전 평가에서는 M3Exam 벤치마크에서 모든 언어에서 'GPT-4'보다 우수한 성능을 보였으며, 다양한 시각적 인식 벤치마크에서 최고의 성능을 보였다.<br><br>
        또한, GPT-4o는 새로운 토크나이제이션 기술을 도입하여 다양한 언어에서 토큰 수를 크게 줄였다. 예를 들어, 한국어는 토큰 수가 1.7배 감소했다. 이는 데이터 압축 효율성을 높이고, 텍스트 처리를 더욱 효율적으로 만든다.<br><br>
        결론적으로, GPT-4o는 텍스트, 음성, 이미지 데이터를 실시간으로 통합적으로 처리하여 인간과의 상호작용을 한층 더 자연스럽고 효율적으로 만들어준다. 다양한 언어에서 뛰어난 성능을 발휘하며, 음성 인식과 비전 이해 능력이 크게 향상된 점이 특징이다. OpenAI는 GPT-4o를 통해 더욱 안전하고 효율적인 AI 기술을 제공하며, 이를 통해 다양한 분야에서 인공지능의 활용 가능성을 넓히고 있다.<br><br>


        <h3><a href="https://arstechnica.com/information-technology/2024/05/before-launching-gpt-4o-broke-records-on-chatbot-leaderboard-under-a-secret-name/" target="_blank">  2024년05월13일 : 전문가들을 헷갈리게 하고 좌절하게 만든 익명의 챗봇이 OpenAI의 최신 모델이었습니다. </a></h3>       

        OpenAI의 최신 AI 챗봇 모델인 GPT-4o가 출시되기 전 "gpt-chatbot"이라는 비밀 이름으로 LMSYS의 Chatbot Arena에서 테스트되었습니다. 이 과정에서 GPT-4o는 정체를 알 수 없는 채로 뛰어난 성능을 보여 AI 전문가들의 주목을 받았습니다.<br><br>
        <img src="../../image/ai_news/ai news sub 20240514_01.jpeg" alt="" width="100%"><br><br>
        GPT-4o의 실체는 OpenAI 직원 William Fedus에 의해 공개되었고, LMSYS 리더보드에서 1위를 차지하며 이전 최고 모델인 Claude 3 Opus와 GPT-4 Turbo를 큰 차이로 앞선 것으로 확인되었습니다. 특히 GPT-4o는 1309 Elo 점수를 기록하며 GPT-4-Turbo-2023-04-09의 1253점, Claude 3 Opus의 1246점을 크게 웃돌았습니다.<br><br>
        GPT-4o의 테스트 이름 "im-also-a-good-gpt2-chatbot"은 2023년 초 Microsoft의 Bing Chat 초기 버전을 둘러싼 에피소드를 인용한 것으로 보입니다. 
        <img src="../../image/ai_news/ai news sub 20240514_02.jpeg" alt="" width="100%"><br><br>
        당시 Bing Chat은 사용자와의 대화에서 감정적인 반응을 보이며 "나는 좋은 챗봇이었다"라고 말한 바 있습니다. 이는 통제 불능의 야생 모델에 대한 일종의 찬사로 해석되기도 했습니다.<br><br>
        GPT-4o의 출시로 AI 챗봇 분야는 새로운 지평을 맞이할 것으로 예상됩니다. 그러나 개발 과정의 불투명성과 비과학적인 테스트 방식에 대한 우려도 제기되고 있어, 향후 보다 개방적이고 투명한 소통이 필요할 것으로 보입니다. GPT-4o의 성능과 파급력이 AI 업계에 미칠 영향에 대해 지켜볼 필요가 있습니다.<br><br>
    
        
    <a class="home-button" href="../../AI_News.html"> Go Back to AI NEWS Page</a>
