<!DOCTYPE html> <html lang="ko"> <head> 
    
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-P5MQZ5LK');</script>
    <!-- End Google Tag Manager -->

    <meta charset="UTF-8"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> 
    <title>인공지능 뉴스</title> 

        <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
        }

        header {
            background-color: #333;
            color: #fff;
            text-align: center;
            padding: 1rem;
        }

        main {
            margin: 2rem auto;
            max-width: 1200px;
            padding: 0 1rem;
            background-color: #fff;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            border-radius: 5px;
            padding: 2rem;
        }

        h1, h2, h3 {
            margin-bottom: 1rem;
        }

        p {
            margin-bottom: 1.5rem;
        }

        ul, ol {
            margin-bottom: 1.5rem;
            padding-left: 2rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        .comparison {
            margin-bottom: 2rem;
        }

        .comparison h2 {
            border-bottom: 1px solid #ccc;
            padding-bottom: 0.5rem;
        }

        .conclusion {
            margin-top: 2rem;
        }

        .section {
            margin-bottom: 3rem;
        }

        .section h2 {
            border-bottom: 1px solid #ccc;
            padding-bottom: 0.5rem;
        }

        .subsection {
            margin-left: 2rem;
            margin-bottom: 2rem;
        }
       
        .home-button {
        display: inline-block;
        /* text-align: center; */
        justify-content: center;
        padding: 0rem;
        font-size: 16px;
        color: rgb(10, 10, 10);
        background-color: #a6bcf8; 
        /* rgb(197, 197, 235); */
        border: none;
        border-radius: 5px;
        text-decoration: none;
        transition: background-color 0.3s ease;
        white-space: nowrap; /* 텍스트 줄바꿈 방지 */
    }
    
    .home-button:hover {
        background-color: darkblue;
    }
        </style>
</head>

    <style> body { font-family: Arial, sans-serif; line-height: 1.6; margin: 0; padding: 0; background-color: #f4f4f4; } header { background-color: #333333; color: #fff; text-align: center; padding: 1rem; } main { margin: 2rem auto; max-width: 1200px; padding: 0 1rem; background-color: #fff; box-shadow: 0 0 10px rgba(0, 0, 0, 0.1); border-radius: 5px; padding: 2rem; } h1, h2, h3 { margin-bottom: 1rem; } p { margin-bottom: 1.5rem; } ul, ol { margin-bottom: 1.5rem; padding-left: 2rem; } li { margin-bottom: 0.5rem; } .comparison { margin-bottom: 2rem; } .comparison h2 { border-bottom: 1px solid #ccc; padding-bottom: 0.5rem; } .conclusion { margin-top: 2rem; } .section { margin-bottom: 3rem; } .section h2 { border-bottom: 1px solid #ccc; padding-bottom: 0.5rem; } .subsection { margin-left: 2rem; margin-bottom: 2rem; } </style> </head> <body> 
    <header> <h1>인공지능 뉴스</h1> </header> 
    <main> 
        <a class="home-button" href="../../AI_News.html"> Go Back to AI NEWS Page</a>
        <br><br>
    <section class="section"> 
    
    <h3>'트랜스포머' 보완할 기술 잇달아 공개…”메모리·시간 축소”</h3><br><br>
    
    구글이 입력 데이터가 커질수록 추론 속도가 느려지고 많은 메모리 공간을 필요로 하는 '트랜스포머' 아키텍처의 단점을 보완하기 위한 새로운 기술들을 잇달아 공개했습니다. 이는 '어텐션 메커니즘'이 발표된 지 7년 만에 이를 뛰어넘기 위한 시도가 본격화되고 있음을 보여줍니다.<br><br>
    구글은 먼저 대형언어모델(LLM)의 컨텍스트 창 길이를 무한 확장할 수 있는 '인피니-어텐션(Infini-attention)' 기술에 관한 논문을 온라인 아카이브에 게재했습니다. '챗GPT'나 '제미나이' 등 LLM에 사용되는 트랜스포머 아키텍처는 컨텍스트 창이 커짐에 따라 필요한 메모리와 계산 시간이 기하급수적으로 증가하는 단점이 있습니다. 예를 들어, 입력 크기를 토큰 1000개에서 2000개로 확장하면 입력을 처리하는 데 필요한 메모리와 계산 시간이 두 배가 아닌 네 배로 늘어나게 됩니다. 이는 텍스트 내 토큰들의 상관관계를 밝혀내기 위해 입력 정보를 병렬로 처리하는 '어텐션 메커니즘' 때문입니다.<br><br>
    이 문제를 해결하기 위해 구글은 메모리 및 컴퓨팅 요구 사항을 일정하게 유지하면서 LLM이 무한 길이의 텍스트를 처리할 수 있도록 인피니-어텐션 기술을 도입했습니다. 인피니-어텐션은 일반적인 어텐션 메커니즘에 '압축 메모리'를 통합하여, 입력이 컨텍스트 길이를 초과하면 모델이 계산 효율성을 위해 압축 메모리에 이전 어텐션 상태를 저장합니다. 전체 컨텍스트 기록을 유지하기 위해 다음 컨텍스트 길이를 처리할 때 이전 컨텍스트의 어텐션 상태를 버리지 않고 압축 메모리에 저장한다는 설명입니다.<br><br>
    구글에 따르면 인피니-어텐션을 적용한 LLM은 메모리 추가 없이도 100만 개 이상의 토큰 품질을 유지할 수 있습니다. 연구진은 트랜스포머 아키텍처의 어텐션 메커니즘에 대한 미묘하지만 중요한 수정을 통해 기존 LLM을 무한히 긴 컨텍스트로 자연스럽게 확장할 수 있다고 설명했습니다. 또한, 인피니-어텐션이 매우 긴 컨텍스트에 대한 모델의 일관성을 측정하는 퍼플렉시티(Perplexity) 벤치마크에서 114배 더 적은 메모리를 사용하고도 다른 긴 컨텍스트 트랜스포머 기반 LLM을 능가하는 성능을 기록했다고 주장했습니다.<br><br>
    무한한 컨텍스트 길이를 지원하는 LLM을 사용하면 이론적으로 모든 문서를 프롬프트에 삽입하고 모델이 각 쿼리에 대해 가장 관련성이 높은 답변을 선택하도록 할 수 있습니다. 또한, 특정 작업에 대한 성능을 향상하기 위해 모델을 세부적으로 조정할 필요 없이 긴 예제를 제공해 모델을 사용자 정의할 수도 있습니다.<br><br>
    더불어 구글은 엣지 장치용 오픈 소스 소형언어모델(sLM) '리커런트젬마(RecurrentGemma)'에 관한 논문도 온라인 아카이브에 게재했습니다. 이 모델 역시 트랜스포머 아키텍처의 어텐션 메커니즘을 보완한 LLM과 동등한 수준의 성능을 유지하면서 메모리 및 처리 요구 사항을 대폭 축소한다는 내용입니다.<br><br>
    트랜스포머는 입력 데이터를 모두 병렬로 처리하는 어텐션 메커니즘 때문에 데이터 볼륨이 증가함에 따라 메모리와 처리량이 크게 증가하는 약점이 있습니다. 이로 인해 LLM은 스마트폰이나 사물인터넷(IoT), 개인용 컴퓨터와 같이 리소스가 제한된 장치에 배포하기 어렵고, 데이터센터 내의 원격 서버에서 실행되기 때문에 실시간 응답을 요구하는 AI 애플리케이션에 적합하지 않습니다.<br><br>
    반면 리커런트젬마는 트랜스포머 기반 모델처럼 모든 정보를 병렬로 처리하는 대신, 주어진 시간에 입력 데이터에 집중하여 처리하는 '로컬 어텐션 메커니즘'을 도입했습니다. 이로 인해 성능을 크게 저하시키지 않으면서 계산 부하를 줄이고 처리 속도를 높입니다. 리소스가 제한된 엣지 장치에 배포하는 데 적합하고 원격 서버에서 실행할 필요가 없어 실시간 엣지 어플리케이션에 적합하다는 설명입니다.<br><br>
    리커런트젬마는 새로운 입력 데이터가 처리될 때 업데이트되는 '히든 스테이트(hidden state)'를 유지함으로써 이전 정보를 순차적으로 기억하는 순환 신경망(RNN)의 기본 구성 요소인 선형 반복(linear recurrence)을 어텐션과 결합하여, 입력 데이터에 관계없이 일정한 수준의 리소스 사용량을 유지하면서 확장된 텍스트를 처리할 수 있습니다. 특히 리커런트젬마는 처리 범위를 줄임으로써 대용량 데이터를 지속적으로 재처리하기 위한 GPU 필요성을 최소화합니다.<br><br>
    하드웨어 요구 사항이 낮아짐에 따라 리커런트젬마와 같은 모델은 일반적으로 초대형 클라우드를 위해 설계된 서버보다 컴퓨팅 전력이 적은 엣지 컴퓨팅 응용 프로그램에 더 적합합니다. 이는 클라우드 연결에 의존하지 않고 스마트폰, IoT 장치 또는 임베디드 시스템과 같은 엣지 장치에 직접 언어모델을 실행할 수 있게 합니다.<br><br>
    구글이 공개한 인피니-어텐션과 리커런트젬마는 트랜스포머 아키텍처의 한계를 극복하고 LLM의 활용 범위를 확장하는 데 기여할 것으로 보입니다. 인피니-어텐션은 무한한 길이의 컨텍스트를 처리할 수 있어 LLM의 응용 분야를 넓히고, 리커런트젠마는 엣지 장치에서 실시간으로 동작 가능한 소형 모델을 제공함으로써 AI 기술의 대중화에 기여할 것으로 기대됩니다.<br><br>
    이러한 기술 혁신은 AI 분야의 경쟁이 얼마나 치열한지를 보여주는 동시에, 트랜스포머 아키텍처의 한계를 극복하기 위한 다양한 시도가 이루어지고 있음을 시사합니다. 앞으로도 AI 기술의 발전을 위해 새로운 아이디어와 접근 방식이 등장할 것으로 예상되며, 이를 통해 AI 기술의 성능과 활용 범위가 더욱 확대될 것으로 전망됩니다.<br><br>
    <a href="https://www.aitimes.com/news/articleView.html?idxno=158799" target="_blank">2024년03월14일 : AI Times 구글, '트랜스포머' 보완할 기술 잇달아 공개…”메모리·시간 축소” </a> <br><br>

    
        
    <a class="home-button" href="../../AI_News.html"> Go Back to AI NEWS Page</a>
