<!--  20240416 0624 만듬 -->
<!DOCTYPE html>

<html lang="ko">
<head>
<meta charset="utf-8"/>
<title>
    인공지능 뉴스 (AI News)
  </title>
<style>
   body {
            font-family: Arial, sans-serif;
        }

        header {
            background-color: #f8f9fa;
            padding: 20px;
            text-align: center;
        }

        main {
            margin: 2rem auto;
            max-width: 1200px;
            padding: 0 1rem;
            background-color: #fff;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            border-radius: 5px;
            padding: 2rem;
            font-size: 40px;
        }
            @media screen and (max-width: 768px) {
            main {
                font-size: 36px;
            }
        }
        ul {
            list-style-type: none;
            padding: 0;
        }

        li {
            margin-bottom: 10px;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 4px;
        }

        footer {
            background-color: #f8f9fa;
            padding: 20px;
            text-align: center;
            /* position: fixed; */
            position: center;
            width: 100%;
            bottom: 0;
        }
  </style>
<style>
   .home-button {
    display: inline-block;
    padding: 20px 100px;
    font-size: 64px;
    color: white;
    background-color: blue;
    border: none;
    border-radius: 5px;
    text-decoration: none;
    transition: background-color 0.3s ease;
}

.home-button:hover {
    background-color: darkblue;
}
  </style>
<style>
   .circle {
    position: absolute;
    height: 100px;
    width: 100px;
    border-radius: 50%;
    border: 5px solid;
    animation: circleFadeOut 1s forwards;
}

@keyframes circleFadeOut {
    from {transform: scale(0.5); opacity: 1;}
    to {transform: scale(1.5); opacity: 0;}
}
  </style>
<style>
   .message {
    position: absolute;
    font-size: 20px;
    color: red;
    animation: messageFadeOut 1s forwards;
}

@keyframes messageFadeOut {
    from {opacity: 1;}
    to {opacity: 0;}
}
  </style>
</head>

<header>
<a class="home-button" href="index.html">
    Home
   </a>
<h1>
    인공지능 뉴스
   </h1>
<p>
    인공지능 뉴스 웹페이지입니다.
    
   </p>
</header>
<main>

<h2>
    인공지능 뉴스
    </h2>


    <ul>
        <li>
        <p>
        <strong><h2></h3>마이크로소프트, 규모는 작아도 성능은 더 뛰어난 새로운 소형 언어 모델 개발</h3><br><br>
                </strong>
                때로는 복잡한 문제를 해결하는 가장 좋은 방법은 동화책에서 힌트를 얻는 것이다. 이는 마이크로소프트 연구원들이 더 작은 패키지에 더 강력한 성능을 담는 방법을 고안하면서 얻은 교훈이다.<br><br>
                <img src="../../image/ai_news/phi3.jpg" alt="" width="100%">
                때로는 복잡한 문제를 해결하는 가장 좋은 방법은 동화책에서 힌트를 얻는 것이다. 이는 마이크로소프트 연구원들이 더 작은 패키지에 더 강력한 성능을 담는 방법을 고안하면서 얻은 교훈이다.<br><br>
                작년, 기계 학습의 수수께끼에 대한 잠재적 해결책을 하루 종일 고민한 후, 마이크로소프트의 로넨 엘단은 딸에게 자장가를 읽어주면서 "그녀가 이 단어를 어떻게 배웠지? 이 단어들을 어떻게 연결하는 걸까?"라고 자문했다.<br><br>
                이는 마이크로소프트 리서치의 기계 학습 전문가로 하여금 AI 모델이 4세 아동이 이해할 수 있는 단어만을 사용해서 얼마나 많이 배울 수 있을지 궁금증을 갖게 했고, 궁극적으로 더 많은 사람들이 AI를 더 쉽게 접근할 수 있도록 하는 새로운 유형의 더 유능한 소형 언어 모델을 만들어내는 혁신적인 학습 접근법으로 이어졌다.<br><br>
                대형 언어 모델(LLMs)은 AI를 사용하여 더 생산적이고 창의적일 수 있는 흥미로운 새로운 기회를 창출했다. 하지만 그 크기로 인해 작동에 상당한 컴퓨팅 자원이 필요할 수 있다.<br><br>
                마이크로소프트는 이러한 모델이 여전히 많은 유형의 복잡한 작업을 해결하는 데 있어 최고의 기준이 될 것이라는 점을 인정하면서도, LLMs에서 발견되는 많은 동일한 기능을 제공하지만 크기가 작고 더 적은 양의 데이터로 학습되는 일련의 소형 언어 모델(SLMs)을 개발해 왔다.<br><br>
                마이크로소프트는 오늘 마이크로소프트 연구원들이 개발한 학습 혁신 덕분에 언어, 코딩 및 수학 능력을 평가하는 다양한 벤치마크에서 동일 크기 및 그 다음 크기의 모델을 능가하는 가장 유능하고 비용 효율적인 소형 언어 모델인 Phi-3 제품군의 공개 모델을 발표했다.<br><br>
                마이크로소프트는 이제 그 크기가 두 배인 모델보다 성능이 우수한 38억 개의 매개변수를 측정하는 Phi-3-mini를 더 강력한 소형 언어 모델 제품군 중 첫 번째로 공개한다고 밝혔다.<br><br>
                오늘부터 Phi-3-mini는 마이크로소프트 Azure AI 모델 카탈로그와 기계 학습 모델 플랫폼인 Hugging Face, 그리고 로컬 머신에서 모델을 실행하기 위한 경량 프레임워크인 Ollama에서 사용할 수 있다. 또한 어디서나 배포할 수 있는 표준 API 인터페이스를 갖춘 NVIDIA NIM 마이크로서비스로도 제공될 예정이다.<br><br>
                마이크로소프트는 또한 품질과 비용 면에서 더 많은 선택권을 제공하기 위해 Phi-3 제품군에 추가 모델이 곧 출시될 것이라고 발표했다. Phi-3-small (70억 개의 매개변수)과 Phi-3-medium (140억 개의 매개변수)은 Azure AI 모델 카탈로그 및 기타 모델 가든에서 곧 사용할 수 있게 될 것이다.<br><br>
                소형 언어 모델은 더 단순한 작업에서 잘 수행되도록 설계되었으며, 제한된 리소스를 가진 조직에서 보다 접근하기 쉽고 사용하기 쉬우며, 특정 요구 사항에 맞게 미세 조정하기가 더 용이하다.<br><br>
                마이크로소프트의 생성형 AI 담당 수석 제품 관리자인 소날리 야다브는 "우리가 앞으로 보게 될 것은 대형에서 소형으로의 전환이 아니라, 단일 범주의 모델에서 고객이 자신의 시나리오에 가장 적합한 모델을 결정할 수 있는 능력을 갖춘 모델 포트폴리오로의 전환"이라고 말했다.<br><br>
                마이크로소프트의 AI 부사장인 루이스 바르가스는 "일부 고객은 소형 모델만 필요로 할 수 있고, 일부는 대형 모델이 필요할 것이며, 많은 고객들이 다양한 방식으로 둘을 결합하기를 원할 것"이라고 말했다.<br><br>
                적합한 언어 모델을 선택하는 것은 조직의 특정 요구 사항, 작업의 복잡성, 가용 자원에 달려 있다. 소형 언어 모델은 클라우드가 아닌 기기에서 로컬로 실행할 수 있는 애플리케이션을 구축하고자 하는 조직과 광범위한 추론이 필요하지 않거나 빠른 응답이 필요한 작업에 적합하다.<br><br>
                소형 언어 모델은 또한 고품질의 결과가 필요하지만 자체 사내에서 데이터를 보관하고자 하는 상황에 직면하는 규제 대상 산업 및 분야에 대한 잠재적 솔루션을 제공한다고 야다브는 말했다.<br><br>
                바르가스와 야다브는 클라우드에 연결되지 않은 "에지"에서 작동하는 스마트폰 및 기타 모바일 기기에 더 유능한 SLM을 배치할 수 있는 기회에 특히 흥분하고 있다. (자동차 컴퓨터, Wi-Fi가 없는 PC, 교통 시스템, 공장 현장의 스마트 센서, 원격 카메라 또는 환경 규정 준수를 모니터링하는 장치를 생각해보라.) 데이터를 기기 내에 보관함으로써 사용자는 "지연 시간을 최소화하고 개인 정보 보호를 극대화할 수 있다"고 바르가스는 말했다.<br><br>
                지연 시간은 LLM이 사용자 프롬프트에 대한 답변을 생성하는 데 사용되는 정보를 검색하기 위해 클라우드와 통신할 때 발생할 수 있는 지연을 의미한다. 경우에 따라 고품질의 답변을 기다릴 가치가 있지만, 다른 시나리오에서는 속도가 사용자 만족도에 더 중요하다.<br><br>
                SLM은 오프라인에서 작동할 수 있기 때문에 더 많은 사람들이 이전에는 불가능했던 방식으로 AI를 활용할 수 있게 될 것이라고 바르가스는 말했다.<br><br>
                예를 들어, SLM은 셀룰러 서비스가 부족한 시골 지역에서 사용될 수 있다. 작물을 검사하는 농부가 잎이나 가지에서 병의 징후를 발견한 경우를 생각해 보자. 시각적 기능을 갖춘 SLM을 사용하여 농부는 문제의 작물 사진을 찍고 해충이나 질병을 치료하는 방법에 대한 즉각적인 권장 사항을 받을 수 있다.<br><br>
                바르가스는 "좋은 네트워크가 없는 세계의 일부 지역에 있다면, 여전히 기기에서 AI 경험을 할 수 있을 것"이라고 말했다.<br><br>
                고품질 데이터의 역할<br><br>
                이름에서 알 수 있듯이 SLM은 LLM에 비해 매우 작다. 적어도 AI 기준으로는 그렇다. Phi-3-mini는 모델의 출력을 결정하는 데 도움이 되는 알고리즘 노브를 의미하는 측정 단위인 38억 개의 매개변수 "만" 가지고 있다. 반면에 가장 큰 대형 언어 모델은 그 크기가 훨씬 더 크다.<br><br>
                대형 언어 모델에 의해 이뤄진 생성형 AI의 엄청난 발전은 주로 그 절대적인 크기에 의해 가능해진 것으로 여겨져 왔다. 그러나 마이크로소프트 팀은 작은 패키지에서 엄청난 결과를 제공할 수 있는 소형 언어 모델을 개발할 수 있었다. 이 돌파구는 학습 데이터에 대한 매우 선별적인 접근 방식을 통해 가능했으며, 이는 동화책이 작용하는 부분이다.<br><br>
                지금까지 대형 언어 모델을 학습시키는 표준적인 방법은 인터넷에서 대량의 데이터를 사용하는 것이었다. 이는 언어의 뉘앙스를 이해하고 사용자 프롬프트에 대한 지능적인 답변을 생성하는 데 필요한 콘텐츠에 대한 이러한 유형의 모델의 거대한 식욕을 충족시키는 유일한 방법으로 여겨졌다. 그러나 마이크로소프트 연구원들은 다른 아이디어를 가지고 있었다.<br><br>
                마이크로소프트의 생성형 AI 연구 부사장이자 더 유능한 소형 언어 모델 개발을 주도해 온 세바스티앙 뷔벡은 "단순히 원시 웹 데이터에 대해 학습하는 대신, 극도로 높은 품질의 데이터를 찾아보는 것은 어떨까요?"라고 물었다. 그러나 어디에 초점을 맞출 것인가?<br><br>
                엘단의 딸과의 밤마다 읽기 의식에서 영감을 받은 마이크로소프트 연구원들은 명사, 동사, 형용사를 대략 동일한 수로 포함하여 3,000개의 단어로 시작하는 개별 데이터 세트를 만들기로 결정했다. 그런 다음 대형 언어 모델에 목록에서 하나의 명사, 하나의 동사, 하나의 형용사를 사용하여 어린이 이야기를 만들도록 요청했는데, 이는 며칠에 걸쳐 수백만 번 반복되어 수백만 개의 작은 어린이 이야기를 생성했다.<br><br>
                SLM은 … 작업을 완료하기 위해 클라우드로 이동할 필요가 없는 계산에 고유하게 위치합니다.<br><br>
                연구원들은 결과로 생성된 데이터 세트를 "TinyStories"라고 명명하고 이를 사용하여 약 1,000만 개의 매개변수를 가진 매우 작은 언어 모델을 학습시켰다. 놀랍게도 자체 이야기를 만들라는 요청을 받았을 때, TinyStories로 학습된 소형 언어 모델은 완벽한 문법으로 유창한 내러티브를 생성했다.<br><br>
                그 다음, 그들은 실험을 한 단계 더 높였다. 이번에는 더 큰 규모의 연구원 그룹이 교육적 가치와 콘텐츠 품질을 기준으로 필터링된 신중하게 선택된 공개 데이터를 사용하여 Phi-1을 학습시켰다. 공개적으로 사용 가능한 정보를 초기 데이터 세트로 수집한 후, TinyStories에 사용된 것과 유사한 프롬프팅 및 시딩 공식을 사용했지만 한 단계 더 나아가 더 정교하게 만들어 더 넓은 범위의 데이터를 포착할 수 있도록 했다. <br><br>
                높은 품질을 보장하기 위해 결과 콘텐츠를 반복적으로 필터링한 후 추가 합성을 위해 LLM에 다시 공급했다. 이러한 방식으로 몇 주에 걸쳐 더 유능한 SLM을 학습시키기에 충분히 큰 데이터 코퍼스를 구축했다.<br><br>
                연구원들은 이 데이터 세트를 "CodeTextbook"이라고 명명했다.<br><br>
                연구원들은 어려운 개념을 학생에게 분해하는 교사처럼 데이터 선택에 접근함으로써 데이터 세트를 더욱 향상시켰다. 뷔벡은 "교과서와 같은 자료, 매우 잘 설명하는 양질의 문서를 읽기 때문에 언어 모델이 이 자료를 읽고 이해하는 작업을 훨씬 더 쉽게 만든다"고 말했다.<br><br>
                고품질 정보와 저품질 정보를 구별하는 것은 인간에게는 어렵지 않지만, 마이크로소프트 연구원들이 SLM을 학습시키는 데 필요하다고 결정한 1테라바이트 이상의 데이터를 살펴보는 것은 LLM의 도움 없이는 불가능할 것이다.<br><br>
                마이크로소프트 리서치 AI 프론티어 랩을 이끌고 새로운 학습 접근법이 개발된 곳의 부사장인 에제 카마르는 "현재 세대의 대형 언어 모델의 힘은 합성 데이터 생성 측면에서 우리가 이전에 가지지 못했던 실제 추진력"이라고 말했다.<br><br>
                신중하게 선택된 데이터로 시작하면 모델이 원치 않거나 부적절한 응답을 반환할 가능성을 줄일 수 있지만, 모든 잠재적 안전 문제를 방지하기에는 충분하지 않다. 모든 생성형 AI 모델 출시와 마찬가지로 마이크로소프트의 제품 및 책임감 있는 AI 팀은 Phi-3 모델 개발에서 위험을 관리하고 완화하기 위해 다층적 접근 방식을 사용했다.<br><br>
                예를 들어, 초기 학습 후 모델이 이상적으로 응답해야 하는 방법에 대한 추가 예제와 피드백을 제공했는데, 이는 추가적인 안전 계층을 구축하고 모델이 고품질 결과를 생성하는 데 도움이 된다. 각 모델은 또한 평가, 테스트 및 수동 레드팀을 거치는데, 이는 전문가가 잠재적 취약점을 식별하고 해결하는 과정이다.<br><br>
                마지막으로 Phi-3 모델 제품군을 사용하는 개발자는 또한 Azure AI에서 사용할 수 있는 도구 모음을 활용하여 더 안전하고 신뢰할 수 있는 애플리케이션을 구축하는 데 도움을 받을 수 있다.<br><br>
                적절한 작업에 적합한 크기의 언어 모델 선택하기<br><br>
                그러나 고품질 데이터로 학습된 소형 언어 모델에도 한계가 있다. 이들은 LLM이 더 큰 용량과 훨씬 더 큰 데이터 세트를 사용한 학습으로 인해 뛰어난 심층 지식 검색을 위해 설계되지 않았다.<br><br>
                LLM은 크기와 처리 능력으로 인해 방대한 양의 정보에 대한 복잡한 추론에서 SLM보다 우수하다. 이는 예를 들어 방대한 과학 논문을 살펴보고, 복잡한 패턴을 분석하며, 유전자, 단백질 또는 화학 물질 간의 상호 작용을 이해하는 데 도움을 줌으로써 신약 개발과 관련될 수 있는 기능이다.<br><br>
                바르가스는 "작업이 있고, 그 작업이 충분히 복잡해서 해당 작업을 여러 하위 작업으로, 때로는 하위-하위 작업으로 분할하는 방법을 알아내야 하고, 최종 답변을 제시하기 위해 이 모든 작업을 실행해야 하는 계획과 같은 것은 당분간 대형 모델의 영역에 있을 것"이라고 말했다.<br><br>
                고객과의 지속적인 대화를 바탕으로 바르가스와 야다브는 일부 기업이 작업이 너무 복잡하지 않은 경우 일부 작업을 소형 모델로 "오프로딩"하는 것을 볼 것으로 예상한다.<br><br>
                예를 들어, 기업은 Phi-3를 사용하여 긴 문서의 주요 내용을 요약하거나 시장 조사 보고서에서 관련 통찰력과 산업 동향을 추출할 수 있다. 다른 조직은 Phi-3를 사용하여 제품 설명이나 소셜 미디어 게시물과 같은 마케팅이나 영업 팀을 위한 콘텐츠 생성에 도움을 줄 수 있다. 또는 기업이 Phi-3를 사용하여 고객의 기본적인 요금제 관련 질문이나 서비스 업그레이드에 대해 답변하는 지원 챗봇에 동력을 공급할 수 있다.<br><br>
                내부적으로 마이크로소프트는 이미 대형 언어 모델이 라우터 역할을 하는 모델 제품군을 사용하고 있는데, 더 적은 컴퓨팅 성능을 요구하는 특정 쿼리를 소형 언어 모델로 전달하는 한편, 다른 더 복잡한 요청은 스스로 처리한다.<br><br>
                "여기서 주장은 SLM이 대형 언어 모델을 대체하거나 교체할 것이라는 게 아니다"라고 카마르는 말했다. 대신 SLM은 "에지에서의 계산, 기기에서의 계산, 작업을 완료하기 위해 클라우드로 이동할 필요가 없는 계산에 고유하게 위치한다. 그렇기 때문에 이 모델 포트폴리오의 장단점을 이해하는 것이 우리에게 중요하다"라고 설명했다.<br><br>
                그리고 크기는 중요한 장점을 지닌다. 소형 언어 모델과 클라우드의 대형 모델에서 얻을 수 있는 지능 수준 사이에는 여전히 차이가 있다고 뷔벡은 말했다. "그리고 아마도 항상 차이가 있을 것이다. 대형 모델도 계속 진전을 이룰 것이기 때문이다."<br><br>
                <a href="https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/" target="_blank"> 2024년04월25일 : MICROSOFT Tiny but mighty: The Phi-3 small language models with big potential</a> <br>  
        </p>

        </li>
        <!-- 이곳에 더 많은 표현을 추가할 수 있습니다 -->
        </ul>

    <ul>
        <li>
        <p>
        <h3>생성형 AI에서 빠진 조각, RAG가 완성한다</h3><br><br>
                
                전 세계의 주목을 받으며 등장한 생성형 AI를 기업에서 구현하는 과정은 시행착오의 연속이었다. AI 채팅 도구가 일상적 업무에 활용되면서 기업 내 섀도우 AI 사용률이 급증하고 있지만, 지식 집약적 워크플로우 방면에서 생성형 AI는 아직 업무 방식을 혁신한다는 원대한 약속을 이행하지 못하고 있다.<br><br>
                <img src="../../image/ai_news/RAG1.jpg" alt="" width="100%">
                그러나 검색 증강 생성(RAG)이라는 프로세스 덕분에 이러한 환멸의 시기가 오래 지속되지는 않을 것으로 보인다. RAG는 과거에는 불가능했던 기업 내 생성형 AI 사용례를 만들어내고 있으며, 오픈AI, 마이크로소프트, 메타, 구글, 아마존과 많은 AI 신생업체들이 엔터프라이즈 중심의 RAG 기반 솔루션을 공격적으로 출시하고 있다.<br><br>
                RAG는 기업에서 생성형 AI의 발목을 잡고 있던 한 가지 큰 문제, 즉 정보 검색 모델을 해결한다. 이제 생성형 AI 도구는 대형 언어 모델(LLM)이 학습한 데이터 외의 관련 데이터에 액세스할 수 있는 방법을 갖게 되었으며, 해당 정보를 기반으로 결과물을 생성할 수 있게 되었다. 단순해 보이지만 이는 기업 사용례에서 생성형 AI 도구의 잠재력을 발휘하는 핵심 기능이다.<br><br>
                생성형 AI가 학습 데이터 외부의 정보에 액세스할 수 없을 때 발생하는 문제를 살펴보면, 챗GPT와 같은 생성형 AI 도구는 LLM을 기반으로 하는데, 이는 특정 시점에 수집된 제한된 범위의 데이터로 학습되어 도메인별 또는 최신 데이터가 부족하다는 한계가 있다. 또한 LLM은 학습한 언어 패턴을 기반으로 새로운 정보를 생성하면서 신뢰할 수 있는 것처럼 보이는 사실을 만들어내는 환각 문제가 있어, 정확성이 중요한 엔터프라이즈 워크플로우에서 걸림돌로 작용하고 있다.<br><br>
                기업에서 필요로 하는 생성형 AI는 첫째, 모든 관련 최신 도메인별 데이터를 포함하여 포괄적이고 시의적절해야 하고, 둘째, 결과물에 사용된 모든 출처를 인용하여 신뢰할 수 있고 투명해야 하며, 셋째, LLM 학습 데이터가 아닌 신뢰할 수 있는 특정 데이터 세트에 기반하여 정확해야 한다.<br><br>
                RAG는 이러한 요구사항을 충족시킬 수 있다. RAG 기반 시스템은 검색 기반 모델과 생성 모델을 통합하여, 대량의 불완전한 비정형 데이터에서 정확한 요약과 인사이트를 추출해 자연어로 명확하고 정확하게 제시하는 지식 집약적인 워크플로우를 처리할 수 있다.<br><br>
                RAG의 4가지 단계는 다음과 같다. 첫째, 신뢰할 수 있는 소스에서 관련 텍스트 정보를 시스템이 분류에 사용할 수 있는 특수 코드로 변환하는 벡터화, 둘째, 수학적 표현을 사용하여 쿼리를 신뢰할 수 있는 정보 소스에 포함된 유사한 코드와 일치시키는 검색, 셋째, 질문 내용, 사용자, 정보 출처를 고려하여 가장 유용한 정보를 선택하는 순위 매기기, 넷째, 해당 문서에서 가장 관련성이 높은 부분을 질문과 결합하여 LLM에 공급하여 결과물을 생성하는 생성 단계이다.<br><br>
                RAG 기반 생성형 AI 도구는 기초 데이터를 적절히 소싱하고 검증하는 한, LLM에만 의존하는 도구보다 훨씬 더 정확하고 포괄적이며 관련성 높은 결과물을 생성할 수 있다. 기업 사용자도 출력된 결과를 신뢰하고 중요한 워크플로우에 활용할 수 있을 것이다.<br><br>
                이에 오픈AI는 챗GPT에 RAG 기능을 도입하기 시작했고, 퍼플렉시티 AI 같은 최신 검색 도구는 원전을 인용하는 응답으로 인기를 얻고 있다. 그러나 여전히 도메인별 엔터프라이즈 사용례에 맞게 작동하려면 시간과 투자가 더 필요한 상황이다.<br><br>
                기업용으로 준비한다는 것은 기초 데이터를 도메인별로 소싱 및 조사하고, 검색을 사용자 정의하며, 사용례와 가장 관련성이 높은 문서를 반환하도록 검색 순위를 매기고, 출력에 올바른 용어, 어조 및 형식을 사용하도록 LLM을 미세 조정하는 것을 의미한다.<br><br>
                생성형 AI에 대한 초기의 뜨거운 관심에도 불구하고, 지금까지 기업에서의 실제 도입은 저조했다. 그러나 RAG는 정확성, 신뢰성, 도메인 특이성이 요구되는 분야에서 생성형 AI 솔루션을 제공함으로써 산업 전반의 판도를 바꾸고 있다. RAG의 등장으로 엔터프라이즈에서 생성형 AI 실현의 새로운 기회가 열리고 있는 것이다.<br><br>
                <a href="https://www.itworld.co.kr/news/334800" target="_blank"> 2024년04월25일 : ITWORLD 생성형 AI에서 빠진 조각, RAG가 완성한다</a> <br>  
        </p>

        </li>
        <!-- 이곳에 더 많은 표현을 추가할 수 있습니다 -->
        </ul>


    <ul>
        <li>
        <p>
        <strong>
                </strong>
                <h3>'토큰 3조개' 익힌 텐센트 AI, 챗GPT보다 학습속도 빨라</h3><br><br>
                
                중국 최대 정보기술(IT) 기업인 텐센트가 개발한 인공지능(AI) 대규모언어모델(LLM) '훈위안'이 최근 학습한 토큰(말뭉치) 수가 3조 개에 달한다는 사실이 알려졌다. 이는 지난해 9월 훈위안 첫 공개 당시 밝힌 2조 개에서 불과 7개월 만에 50%나 증가한 수치다. 이는 네이버가 2021년 선보인 한국 최초 LLM의 5600억 토큰과 비교해도 압도적인 규모로, '챗GPT의 아버지'로 불리는 샘 올트먼의 오픈AI가 2020년 GPT3 출범 당시 투입한 3000억 개 토큰과 비교해도 훨씬 빠른 속도다. 텐센트의 이러한 빠른 추격에 세계 빅테크들이 경계심을 갖는 이유다.<br><br>
                텐센트가 AI 개발에 본격적으로 뛰어든 것은 2016년으로 거슬러 올라간다. 당시 텐센트는 텐센트AI랩을 설립하고, 자사의 SNS인 위챗과 웨이신을 통해 축적한 방대한 데이터를 AI 학습에 활용하기 시작했다. 지난 18일 방문한 중국 광저우 위챗·웨이신 사무동에 따르면, 현재 이 두 SNS의 이용자 수는 14억 4700만 명에 달한다. 이 중 중국인 이용자가 13억 5900만 명으로 거의 모든 중국인이 사용하고 있으며, 외국인 이용자도 1억 명에 육박한다. 텐센트 관계자는 "대다수 AI 기업이 토큰에 쓸 데이터 부족으로 어려움을 겪고 있지만, 텐센트는 걱정할 필요가 없다"고 말했다. 위챗이 금융 거래부터 진료 예약, 처방전 발송, 식당 결제, 택시 콜, 공유자전거 이용 등 일상생활 전반에 활용되는 '생활 리모컨' 역할을 하기 때문에 AI 학습에 필요한 토큰을 대량으로 확보할 수 있기 때문이다.<br><br>
                AI 성능 개선에 있어 토큰은 가장 확실한 수단으로 꼽힌다. 아무리 최첨단 AI 칩을 사용한다 해도 학습에 투입되는 데이터가 부족하다면 성능 향상에 한계가 있기 마련이다. 반면 칩의 성능이 다소 떨어지더라도 방대한 데이터를 학습한다면 더 나은 결과를 기대할 수 있다. 네이버 관계자는 "오프라인의 실시간 데이터를 대규모로 확보하는 곳은 중국 빅테크뿐"이라고 말했다.<br><br>
                텐센트의 마화텅 회장은 훈위안을 고도화하는 한편, 기업AI로 사업 영역을 확장하고 있다. 그는 최근 임직원에게 보낸 메시지에서 "훈위안은 수치 추론, 논리적 추론, 다단계 대화 등에서 탁월한 성능을 발휘하는 최상위 모델"이라고 강조했다. 훈위안은 이미지 생성, 텍스트 인식, 카피라이팅 등 다양한 기능을 지원하며, SNS, 금융, 공공 서비스, 전자상거래, 물류 운송, 게임 등 주요 산업에서 활용도가 높다. 다우슨 퉁 텐센트 수석부사장은 "금융, 교육, 물류, 게임 등 20개 산업군에 걸쳐 50여 개 솔루션을 보유한 기업용 훈위안 서비스를 중국 기업에 제공하고 있다"며 "각 기업에 최적화된 지능형 서비스를 선보일 것"이라고 말했다.<br><br>
                기업AI는 그동안 중국의 최대 약점으로 꼽혀왔다. 소비 영역과 달리 중국 기업이 자체적으로 축적한 '정형화된 데이터'가 부족했기 때문이다. 텐센트는 자사의 AI 클라우드를 통해 주요 산업의 데이터를 끌어모음으로써 이러한 약점을 극복하겠다는 계획이다. 구글, 아마존, 마이크로소프트 등이 주도하는 초거대 AI 생태계와는 완전히 다른, 별개의 'AI 행성'을 만들겠다는 것이 마화텅 회장의 전략이다. 실제로 텐센트는 지난달 자체 개발한 생성형 AI 게임엔진 '지넥스'를 공개하며 이에 대한 의지를 내비쳤다. 생성형 AI를 활용해 콘텐츠 제작 시간을 단축하고 풍성한 게임 스토리를 만들어낼 수 있는 혁신적인 도구인 셈이다. 텐센트는 현재 훈위안의 기업AI를 자사의 사업 및 서비스에 적용 중인데, 그 분야만 해도 400개에 달한다고 한다.<br><br>
                텐센트는 AI 기술을 바탕으로 글로벌 시장 공략에도 적극적으로 나서고 있다. 2016년 아시아, 유럽, 미국을 아우르는 글로벌 파트너 네트워크를 구축한 것을 시작으로 해외 진출을 본격화했으며, 최근에는 중남미와 중동 지역에서 독자적인 생태계를 구축하는 중이다. 자오젠난 텐센트 클라우드인터내셔널 부사장은 "클라우드, AI, 빅데이터, 보안 등 주요 기술을 통합해 맞춤형 디지털 솔루션을 구축해주는 형태의 글로벌 사업을 키워나갈 것"이라고 밝혔다. 이를 위해 텐센트는 지난해에만 640억 위안(약 12조 원)을 연구개발(R&D)에 투자했는데, 이는 역대 최대 규모다. 올해는 R&D 투자금 중 AI 관련 비중을 더욱 높일 것으로 알려졌다.<br><br>
                중국 IT 공룡 텐센트의 야심찬 행보가 주목되는 이유다. 자국 내 시장에서의 독보적인 지위를 바탕으로 축적한 데이터와 AI 기술력을 앞세워 글로벌 무대에서도 존재감을 과시하겠다는 전략이 엿보인다. 구글, 아마존, 마이크로소프트 등 미국 빅테크 기업들이 주도하는 AI 시장에서 새로운 강자로 부상할 수 있을지 귀추가 주목된다. 다만 중국 정부의 규제와 검열, 데이터 보안 문제 등은 여전히 텐센트가 넘어야 할 과제로 남아있다. 글로벌 시장에서 통할 수 있는 보편적인 가치와 기준을 충족시키는 것도 중요한 숙제다. 텐센트의 행보가 중국 IT 기업의 새로운 가능성을 보여줄 수 있을지, 아니면 자국 시장에 갇힌 한계를 노출할 지 지켜볼 일이다.<br><br>
                <a href="https://www.hankyung.com/article/2024042159721" target="_blank"> 2024년04월21일 : 한국경제 '토큰 3조개' 익힌 텐센트 AI, 챗GPT보다 학습속도 빨라  </a> <br>  
        </p>

        </li>
        <!-- 이곳에 더 많은 표현을 추가할 수 있습니다 -->
        </ul>

    <ul>
        <li>
        <p>
        <strong>
                </strong>
                <h3>“억대 연봉 옛말 됐다”…이 직원 4명분 AI가 도맡아 ‘대규모 칼바람’</h3><br><br>
                인공지능(AI)이 IT 개발자들의 핵심 업무인 코딩을 중급 이상으로 수행할 수 있게 되면서, 기업들은 고연봉 IT 개발자 대신 비용 효율적인 AI에 투자하는 추세로 바뀌고 있습니다. 이로 인해 IT 개발자들의 입지가 흔들리고 있는 상황입니다.<br><br>
                최근 전 세계 기업들은 IT 인재 영입에 들일 자금을 자체 AI 개발에 투자하고 있습니다. 구글, 마이크로소프트, 아마존 등 해외 빅테크 기업들은 대규모 구조조정을 단행하고, 절감된 비용을 AI 개발에 투자하고 있습니다. 구글은 370억 달러(약 51조 원)를, 마이크로소프트는 25억 파운드(약 4조 원)를, 아마존은 40억 달러(약 5조 원)와 1억 달러(약 1,385억 원)를 각각 AI 관련 분야에 투자할 계획입니다.<br><br>
                전문가들은 AI가 머지않아 IT 개발자의 직무를 대체할 수 있을 것으로 전망하고 있습니다. 엔비디아 CEO 젠슨 황은 AI의 발전으로 코딩 전문 직업이 사라질 수 있다고 언급했으며, 서울여대 김명주 교수는 AI를 코딩에 적용하면 효율성이 30~40% 향상될 수 있다고 설명했습니다. 아크 인베스트는 AI가 코딩 생산성을 10배 높일 수 있다고 관측했습니다.<br><br>
                국내에서도 AI로 인한 IT 개발자 일자리 대체 우려가 확산되고 있습니다. 원티드랩의 설문조사 결과, 응답자의 83.6%가 AI가 개발자 업무를 대체할 것이라고 답했습니다. 실제로 국내 IT 개발자 채용은 감소 추세를 보이고 있으며, 사람인의 분석에 따르면 지난해 IT 개발·데이터 직무 공고가 전년 대비 7.4% 감소했습니다.<br><br>
                국내 대형 IT 기업들도 연봉 및 인센티브 인상률을 낮추고, AI에 투자를 집중하고 있습니다. 네이버와 카카오의 직원 평균 연봉은 각각 11.5%, 27.3% 감소했으며, 이는 AI 투자를 늘리기 위한 조치로 보입니다.<br><br>
                전문가들은 기업들이 AI의 효율성을 확인하게 되면, 뛰어난 인재를 제외하고는 새로운 개발자 채용을 줄일 가능성이 높다고 예측합니다. AI의 발전이 IT 개발자의 일자리에 상당한 영향을 미칠 것으로 보이며, 이에 따른 고용 구조의 변화가 예상됩니다.<br><br>
                IT 업계에서는 신입 개발자 채용을 지양하고, 검증된 경력자 위주로 선발하는 추세입니다. 프로젝트성 수시 채용만 운영하는 등 채용 규모 자체를 줄이고 있는 상황입니다.<br><br>
                AI의 발전과 기업들의 투자 전략 변화로 인해, IT 개발자들의 고용 환경이 급변하고 있습니다. 단순 코딩 업무는 AI로 대체되고, 개발자들은 AI를 활용하고 관리하는 역할로 전환될 것으로 보입니다. 이러한 변화에 대응하기 위해서는 개발자들의 역량 강화와 함께, 기업과 정부 차원의 고용 안정 대책 마련이 필요할 것으로 사료됩니다.<br><br>
                <a href="https://m.mk.co.kr/amp/10992831" target="_blank"> 2024년04월17일 : 매일경제 “억대 연봉 옛말 됐다”…이 직원 4명분 AI가 도맡아 ‘대규모 칼바람’  </a> <br>  
        </p>

        </li>
        <!-- 이곳에 더 많은 표현을 추가할 수 있습니다 -->
        </ul>

        <ul>
        <li>
        <p>
        <strong>
                </strong>
                <h3>2024년 가장 많이 사용되는 AI 플랫폼 20개는..</h3><br><br>
                2024년 가장 많이 사용되는 인공지능(AI) 플랫폼 순위가 발표되었습니다. 미국 투자 전문매체 인사이더몽키가 인터넷 상의 10개 이상 순위 사이트를 참고하여, 50% 이상 등장한 플랫폼을 선정하고 지난 28일간의 총 사이트 방문수를 기준으로 순위를 매겼습니다. 웹사이트가 없는 플랫폼의 경우에는 총 구독자 수나 사용자 수를 기준으로 하였습니다.<br><br>
                1위는 오픈AI의 챗GPT가 차지했습니다. 방문수 16억 1,000만 건으로 압도적인 성적을 기록했습니다. 2위는 구글의 제미나이가 3억 9,120만 건으로 뒤를 이었고, 3위는 의외로 번역 AI인 딥엘(DeepL)이 2억 4,130만 건으로 차지했습니다.<br><br>
                4위부터 10위까지는 캐릭터닷AI의 챗봇, 퍼플렉시티 AI의 AI 기반 검색엔진, 앤트로픽의 클로드, 쿼라의 AI 챗봇 포, 마이크로소프트(MS)의 코파일럿, AI 언어모델인 다이얼로GPT, AI 기반 검색엔진 유챗이 각각 자리를 잡았습니다.<br><br>
                톱10 중에서 대형언어모델(LLM)이나 AI 챗봇을 기반으로 하지 않은 플랫폼은 번역 AI인 딥엘이 유일했습니다. MS가 8위에 오른 것은 예상 밖의 결과였으며, 퍼플렉시티 AI와 유챗 등 AI 기반 검색 엔진의 상승세도 주목할 만한 점이었습니다.<br><br>
                11위부터 20위까지는 AI 기반 회의 도우미 오터(Otter), 학생들의 숙제 도우미 소크라틱, 글쓰기 지원 AI 힉스.AI, 라이트소닉, 카피AI, AI 챗봇 재스퍼, 영어 음성 도우미 엘사, 코딩 도우미 깃허브의 코파일럿, 코디엄, 탭나인 등이 차지했습니다.<br><br>
                이번 순위는 AI 플랫폼의 대중적 인기와 사용 현황을 잘 보여주고 있습니다. 챗GPT와 같은 대화형 AI 모델의 폭발적인 인기가 두드러지는 가운데, 번역, 검색, 회의, 학습, 글쓰기, 코딩 등 다양한 분야에서 AI 기술이 적용되고 있음을 알 수 있습니다.<br><br>
                특히 대형 IT 기업들의 AI 플랫폼뿐만 아니라, 스타트업과 특화된 AI 서비스들도 상위권에 포진해 있어, AI 산업의 다양성과 경쟁 구도를 엿볼 수 있습니다. 퍼플렉시티 AI나 유챗 등 신생 AI 검색엔진의 약진도 눈에 띕니다.<br><br>
                AI 기술의 발전과 대중화가 가속화되면서, 이러한 AI 플랫폼들은 우리의 일상과 업무 방식에 점점 더 큰 영향을 미칠 것으로 보입니다. 동시에 AI 윤리, 프라이버시, 일자리 등 사회적 이슈에 대한 논의도 더욱 중요해질 전망입니다.<br><br>
                2024년 AI 플랫폼 순위는 급변하는 AI 기술 생태계의 현주소를 보여주는 동시에, 앞으로의 발전 방향을 가늠해볼 수 있는 흥미로운 자료라 할 수 있겠습니다. AI 기술이 가져올 변화와 기회, 그리고 우리 사회가 준비해야 할 과제들을 생각해 볼 수 있는 계기가 될 것 같습니다. <br><br>
                <a href="https://www.aitimes.com/news/articleView.html?idxno=158780" target="_blank">2024년04월14일 : AI Times 2024년 가장 많이 사용되는 AI 플랫폼 20개는 </a> <br>
        </p>

        </li>
        <!-- 이곳에 더 많은 표현을 추가할 수 있습니다 -->
        </ul>

        <ul>
            <li>
            <p>
            <strong>
                    </strong>
                    <h3>"GPT-4 다시 게을러져...클로드 3로 갈아타자" 여론 확산</h3><br><br>
                오픈AI의 챗봇 GPT-4에서 '게으름(laziness)' 문제가 다시 불거지면서, 사용자들 사이에서는 앤트로픽의 경쟁 모델인 '클로드 3'로 갈아타자는 여론이 확산되고 있습니다.
                GPT-4의 게으름 문제는 챗봇이 응답을 거부하거나 정상적으로 작동하지 않는 현상을 의미합니다. 이 문제는 지난해 여름부터 발생했으며, 당시에는 모델 분할 과정인 '전문가 모델(MoE)' 방식 적용으로 인한 일시적 성능 저하로 여겨졌습니다. 그러나 문제가 반복되자 오픈AI는 작년 11월 GPT-4 터보를 출시하여 해결을 시도했습니다.<br><br>
                올해 초에도 유사한 문제가 발생했고, 샘 알트먼 오픈AI CEO가 트위터를 통해 이를 인정하고 수정했다고 밝혔지만, 최근 다시 문제가 불거졌습니다. 사용자들은 개발자 포럼과 SNS 등을 통해 모델의 성능 저하와 불완전한 코드 생성 등에 대한 불만을 토로하고 있습니다.<br><br>
                과거에는 사용자들이 이를 그대로 감수해야 했지만, 이제는 앤트로픽의 '클로드 3'라는 대안이 등장했습니다. 클로드 3는 벤치마크 결과와 인간 선호도 평가에서 GPT-4를 능가하는 성능을 보여주고 있습니다. 엔젤 투자자 앨리 밀러는 대부분의 사람들이 클로드 3를 사용하고 있다고 언급했으며, 이던 몰릭 와튼대학교 교수는 구체적인 사례를 들어 클로드 3의 우수성을 강조했습니다.<br><br>
                오픈AI가 이 문제에 적절히 대응할 수 있을지에 대해서도 의문이 제기되고 있습니다. 밀러는 오픈AI가 다음 모델 개발에 초점을 맞추고 있어 이 문제에 충분한 자원을 투입하지 않을 것으로 예상했습니다. 실제로 최근 보도에 따르면 오픈AI는 올 여름 GPT-5 출시를 계획하고 있습니다.<br><br>
                이번 사태는 AI 챗봇 시장에서 경쟁이 심화되고 있음을 보여줍니다. GPT-4의 게으름 문제가 지속될 경우, 사용자들의 이탈이 가속화되고 클로드 3와 같은 경쟁 모델의 점유율이 확대될 가능성이 있습니다. 오픈AI가 이 문제를 신속하고 효과적으로 해결하지 않는다면, 시장에서의 위상이 흔들릴 수 있다는 우려도 제기됩니다.<br><br>
                한편, 이번 사례는 AI 기술의 발전과 상용화 과정에서 발생할 수 있는 다양한 기술적, 사회적 이슈를 보여주고 있습니다. 챗봇의 성능과 안정성, 사용자 경험, 기업의 대응 능력 등이 중요한 화두로 떠오르고 있으며, 이는 향후 AI 산업의 발전 방향과 속도에 영향을 미칠 것으로 보입니다. AI 기술을 둘러싼 경쟁과 혁신이 가속화되는 가운데, 사용자들의 요구와 기대에 부응하는 서비스를 제공하는 것이 기업들의 핵심 과제로 부상하고 있습니다.<br><br>
                    <a href="https://www.aitimes.com/news/articleView.html?idxno=158387" target="_blank">2024년03월29일 : AI Times "GPT-4 다시 게을러져...클로드 3로 갈아타자" 여론 확산 </a> <br>
            </p>
    
            </li>
            <!-- 이곳에 더 많은 표현을 추가할 수 있습니다 -->
            </ul>

            <ul>
                <li>
                <p>
                <strong>
                        </strong>
                        <h3>'트랜스포머' 보완할 기술 잇달아 공개…”메모리·시간 축소”</h3><br><br>
                구글이 입력 데이터가 커질수록 추론 속도가 느려지고 많은 메모리 공간을 필요로 하는 '트랜스포머' 아키텍처의 단점을 보완하기 위한 새로운 기술들을 잇달아 공개했습니다. 이는 '어텐션 메커니즘'이 발표된 지 7년 만에 이를 뛰어넘기 위한 시도가 본격화되고 있음을 보여줍니다.<br><br>
                구글은 먼저 대형언어모델(LLM)의 컨텍스트 창 길이를 무한 확장할 수 있는 '인피니-어텐션(Infini-attention)' 기술에 관한 논문을 온라인 아카이브에 게재했습니다. '챗GPT'나 '제미나이' 등 LLM에 사용되는 트랜스포머 아키텍처는 컨텍스트 창이 커짐에 따라 필요한 메모리와 계산 시간이 기하급수적으로 증가하는 단점이 있습니다. 예를 들어, 입력 크기를 토큰 1000개에서 2000개로 확장하면 입력을 처리하는 데 필요한 메모리와 계산 시간이 두 배가 아닌 네 배로 늘어나게 됩니다. 이는 텍스트 내 토큰들의 상관관계를 밝혀내기 위해 입력 정보를 병렬로 처리하는 '어텐션 메커니즘' 때문입니다.<br><br>
                이 문제를 해결하기 위해 구글은 메모리 및 컴퓨팅 요구 사항을 일정하게 유지하면서 LLM이 무한 길이의 텍스트를 처리할 수 있도록 인피니-어텐션 기술을 도입했습니다. 인피니-어텐션은 일반적인 어텐션 메커니즘에 '압축 메모리'를 통합하여, 입력이 컨텍스트 길이를 초과하면 모델이 계산 효율성을 위해 압축 메모리에 이전 어텐션 상태를 저장합니다. 전체 컨텍스트 기록을 유지하기 위해 다음 컨텍스트 길이를 처리할 때 이전 컨텍스트의 어텐션 상태를 버리지 않고 압축 메모리에 저장한다는 설명입니다.<br><br>
                구글에 따르면 인피니-어텐션을 적용한 LLM은 메모리 추가 없이도 100만 개 이상의 토큰 품질을 유지할 수 있습니다. 연구진은 트랜스포머 아키텍처의 어텐션 메커니즘에 대한 미묘하지만 중요한 수정을 통해 기존 LLM을 무한히 긴 컨텍스트로 자연스럽게 확장할 수 있다고 설명했습니다. 또한, 인피니-어텐션이 매우 긴 컨텍스트에 대한 모델의 일관성을 측정하는 퍼플렉시티(Perplexity) 벤치마크에서 114배 더 적은 메모리를 사용하고도 다른 긴 컨텍스트 트랜스포머 기반 LLM을 능가하는 성능을 기록했다고 주장했습니다.<br><br>
                무한한 컨텍스트 길이를 지원하는 LLM을 사용하면 이론적으로 모든 문서를 프롬프트에 삽입하고 모델이 각 쿼리에 대해 가장 관련성이 높은 답변을 선택하도록 할 수 있습니다. 또한, 특정 작업에 대한 성능을 향상하기 위해 모델을 세부적으로 조정할 필요 없이 긴 예제를 제공해 모델을 사용자 정의할 수도 있습니다.<br><br>
                더불어 구글은 엣지 장치용 오픈 소스 소형언어모델(sLM) '리커런트젬마(RecurrentGemma)'에 관한 논문도 온라인 아카이브에 게재했습니다. 이 모델 역시 트랜스포머 아키텍처의 어텐션 메커니즘을 보완한 LLM과 동등한 수준의 성능을 유지하면서 메모리 및 처리 요구 사항을 대폭 축소한다는 내용입니다.<br><br>
                트랜스포머는 입력 데이터를 모두 병렬로 처리하는 어텐션 메커니즘 때문에 데이터 볼륨이 증가함에 따라 메모리와 처리량이 크게 증가하는 약점이 있습니다. 이로 인해 LLM은 스마트폰이나 사물인터넷(IoT), 개인용 컴퓨터와 같이 리소스가 제한된 장치에 배포하기 어렵고, 데이터센터 내의 원격 서버에서 실행되기 때문에 실시간 응답을 요구하는 AI 애플리케이션에 적합하지 않습니다.<br><br>
                반면 리커런트젬마는 트랜스포머 기반 모델처럼 모든 정보를 병렬로 처리하는 대신, 주어진 시간에 입력 데이터에 집중하여 처리하는 '로컬 어텐션 메커니즘'을 도입했습니다. 이로 인해 성능을 크게 저하시키지 않으면서 계산 부하를 줄이고 처리 속도를 높입니다. 리소스가 제한된 엣지 장치에 배포하는 데 적합하고 원격 서버에서 실행할 필요가 없어 실시간 엣지 어플리케이션에 적합하다는 설명입니다.<br><br>
                리커런트젬마는 새로운 입력 데이터가 처리될 때 업데이트되는 '히든 스테이트(hidden state)'를 유지함으로써 이전 정보를 순차적으로 기억하는 순환 신경망(RNN)의 기본 구성 요소인 선형 반복(linear recurrence)을 어텐션과 결합하여, 입력 데이터에 관계없이 일정한 수준의 리소스 사용량을 유지하면서 확장된 텍스트를 처리할 수 있습니다. 특히 리커런트젬마는 처리 범위를 줄임으로써 대용량 데이터를 지속적으로 재처리하기 위한 GPU 필요성을 최소화합니다.<br><br>
                하드웨어 요구 사항이 낮아짐에 따라 리커런트젬마와 같은 모델은 일반적으로 초대형 클라우드를 위해 설계된 서버보다 컴퓨팅 전력이 적은 엣지 컴퓨팅 응용 프로그램에 더 적합합니다. 이는 클라우드 연결에 의존하지 않고 스마트폰, IoT 장치 또는 임베디드 시스템과 같은 엣지 장치에 직접 언어모델을 실행할 수 있게 합니다.<br><br>
                구글이 공개한 인피니-어텐션과 리커런트젬마는 트랜스포머 아키텍처의 한계를 극복하고 LLM의 활용 범위를 확장하는 데 기여할 것으로 보입니다. 인피니-어텐션은 무한한 길이의 컨텍스트를 처리할 수 있어 LLM의 응용 분야를 넓히고, 리커런트젠마는 엣지 장치에서 실시간으로 동작 가능한 소형 모델을 제공함으로써 AI 기술의 대중화에 기여할 것으로 기대됩니다.<br><br>
                이러한 기술 혁신은 AI 분야의 경쟁이 얼마나 치열한지를 보여주는 동시에, 트랜스포머 아키텍처의 한계를 극복하기 위한 다양한 시도가 이루어지고 있음을 시사합니다. 앞으로도 AI 기술의 발전을 위해 새로운 아이디어와 접근 방식이 등장할 것으로 예상되며, 이를 통해 AI 기술의 성능과 활용 범위가 더욱 확대될 것으로 전망됩니다.<br><br>
                        <a href="https://www.aitimes.com/news/articleView.html?idxno=158799" target="_blank">2024년03월14일 : AI Times 구글, '트랜스포머' 보완할 기술 잇달아 공개…”메모리·시간 축소” </a> <br><br>
                </p>
        
                </li>
                <!-- 이곳에 더 많은 표현을 추가할 수 있습니다 -->
                </ul>

                <ul>
                    <li>
                    <p>
                    <strong>
                            </strong>
                            <h3>기사 스토리 입력하면 웹툰 만들어주는 생성 AI 출시</h3><br>
                            미국 스타트업 로어 머신(Lore Machine)이 장문의 스토리를 입력하면, 이에 맞춰 다양한 스타일의 이미지나 사운드, 애니메이션을 생성해 웹툰이나 그래픽노블을 만들어 주는 인공지능(AI) 도구를 출시했다.<br><br>
                <img src="../../image/ai_news/ai news sub 20240306_01 lore machine.png" alt="" width="100%">
                로어 머신은 특히 '일관성 유지'를 이 도구의 가장 큰 장점으로 꼽았다.<br><br>
                로이터와 MIT 테크 리뷰 등은 5일(현지시간) 로어 머신이 AI 시각적 스토리텔링 플랫폼을 공개하고, 월 10 ~ 160달러(약 1만3400원 ~ 21만3800원)의 요금제를 내놓았으며, 현재 투자 유치를 진행 중이라고 보도했다.<br><br>
                이 도구는 텍스트 프롬프트에 단편 소설이나 대본 등 최대 3만 단어를 입력하면, 일관성 있는 이미지를 생성해 웹툰 등을 제작할 수 있다. 월 10달러를 지불하면 10만 단어를 업로드해 이미지 80개를 만들 수 있고, 가장 비싼 160달러 요금제는 224만 단어로 1792개의 이미지를 생성해 낸다.<br><br>
                생성되는 이미지는 북미식 카툰부터 일본식 만화, 수채화, 실사형 등으로 다양하게 커스터마이징할 수 있다. 사용자는 스토리보드에 따라 장면을 선택하고 생성해 작품을 완성하게 된다. 이 도구로 만화 한편을 제작하는 데 클릭 열번도 걸리지 않았다는 체험기도 있다.<br><br>
                토비 캠피언 로어 머신 설립자는 크리에이티브 에이전시 모던 아츠의 잭 라이더 설립자로부터 단편 영화 대본을 받은 뒤, 하룻밤 사이에 16페이지 분량의 그래픽 노블을 만들어냈다고 밝혔다.<br><br>
                이를 확인한 라이더 설립자는 "이미지 생성 자체가 핵심이 아니다. 서사부터 등장인물의 감정 묘사까지 모든 것이 스토리텔링에 딱 맞아떨어지는 것이 중요하다"라며 만족감을 표했다. 모던 아츠는 현재 이 도구를 활용해 넷플릭스 인기 애니메이션 '러브, 데스+로봇'의 만화 시리즈를 개발하고 있는 것으로 알려졌다.<br><br>
                로어 머신 개발은 캠피언 설립자가 미드저니를 이용해 코믹북을 제작하던 경험에서 비롯됐다. 이미지 퀄리티는 뛰어나지만, 이미지 간 일관성을 유지하기 어렵다는 문제점을 발견한 것이다. 예를 들면 주인공의 헤어 스타일이 들쭉날쭉해지거나, 특정 스타일을 고정하는 것이 거의 불가능했다.<br><br>
                이에 로어 머신이 개발한 도구의 최대 장점은 일관성 유지에 있다. 이미지 생성에는 스테이블 디퓨전을 활용하고 있으며, 자체 개발한 언어모델도 내장하고 있다. 개발에는 1년이 소요됐다. 폭력적이거나 혐오스러운 이미지 생성은 막았지만, 다른 AI 도구들처럼 환각을 100% 차단하기는 어렵다고 전했다.<br><br>
                로어 머신은 이미 중국의 컴퓨터 업체 광고 제작에 활용되는 등 지난 분기부터 상당한 수익을 거두고 있다고 밝혔다. 현재 20개의 소프트웨어 플랫폼 및 콘텐츠 업체와 파트너십을 논의 중이며, 기업이 자체 스타일로 모델을 교육할 수 있는 플랫폼 버전도 개발하고 있다. 향후에는 단어 입력 제한을 50만개까지 늘릴 계획이다.<br><br>
                로어 머신은 상업적 활용뿐만 아니라 교육 용도로도 많은 요청을 받고 있다고 전했다. 현재 투자 유치를 진행 중이지만, 구체적인 내용은 공개하지 않았다.<br><br>
                        <a href="https://www.aitimes.com/news/articleView.html?idxno=157726" target="_blank"> AI Times 기사 스토리 입력하면 웹툰 만들어주는 생성 AI 출시  </a> <br>  
                        <!-- <a href="https://www.aitimes.com/news/articleView.html?idxno=158799" target="_blank">2024년03월14일 : AI Times 구글, '트랜스포머' 보완할 기술 잇달아 공개…”메모리·시간 축소” </a> <br><br> -->
                    </p>
            
                    </li>
                    <!-- 이곳에 더 많은 표현을 추가할 수 있습니다 -->
                    </ul>

    <style>
        /* Increase the font size on screens smaller than 600px (typical size for mobile phones) */
        @media only screen and (max-width: 600px) {
            body {
                font-size: 3em !important;
            }
        }
        
    </style>

</main>


<footer>
   © 2023~2024 AI NEWS. All rights reserved.
   <a href="mailto:stberry7@gmail.com?subject=Hello&amp;body=Nice%20to%20meet%20you">
    Email me
   </a>
</footer>
<script>
   var clickCount = 0;
  </script>
<style>
.firework {
    position: absolute;
    width: 1px;
    height: 1px;
    border-radius: 50%;
    background: red;
    animation: explode 3s ease-out;
}

@keyframes explode {
    to {
        transform: scale(30);
        opacity: 0;
    }
}
</style>
<script>
var colors = ['#ffb3ba', '#ffdfba', '#ffffba', '#baffc9', '#bae1ff'];

document.addEventListener('click', function(event) {
    if (event.clientX < 30 || event.clientX > window.innerWidth - 30 ||
        event.clientY < 30 || event.clientY > window.innerHeight - 30) {
        return;
    }
    for (var i = 0; i < 10; i++) {
        var firework = document.createElement('div');
        firework.className = 'firework';
        // firework.style.top = (event.clientY - 0.5) + 'px';
        firework.style.top = (event.clientY+ window.scrollY - 0.5) + 'px';
        firework.style.left = (event.clientX - 0.5) + 'px';
        // firework.style.top = (event.clientY + window.scrollY - 0.5) + 'px'; // Adjusted for scroll
        firework.style.backgroundColor = colors[Math.floor(Math.random() * colors.length)];
        firework.style.transform = 'translate(' + (Math.random() * 200 - 100) + 'px, ' + (Math.random() * 200 - 100) + 'px)';
        document.body.appendChild(firework);
        setTimeout(function() {
            document.body.removeChild(firework);
        }, 3000);
    }
});
</script>
</body>
</html>
