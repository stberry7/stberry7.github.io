import asyncio
import aiohttp
from bs4 import BeautifulSoup
from googlesearch import search
import os
import tkinter as tk
from tkinter import filedialog, simpledialog
from datetime import datetime

async def fetch(session, url):
    try:
        print(f"페이지 분석 중: {url}")
        async with session.get(url) as response:
            text = await response.text()
            soup = BeautifulSoup(text, 'html.parser')
            page_text = soup.get_text()
            return url, len(page_text), text.lower()
    except:
        print(f"페이지 분석 실패: {url}")
        return url, 0, ""

async def search_pages(query, exclude_words, min_chars, num_results, start_date):
    result = []
    search_query = " ".join(query)
    search_results = search(search_query, num=num_results)

    print(f"총 {num_results}개의 검색 결과 분석 시작...")

    async with aiohttp.ClientSession() as session:
        tasks = [fetch(session, url) for url in search_results]
        responses = await asyncio.gather(*tasks)

    for i, (url, char_count, page_text) in enumerate(responses, start=1):
        print(f"진행 상황: {i}/{num_results}")

        if char_count > min_chars and all(word.lower() in page_text for word in query) and not any(word.lower() in page_text for word in exclude_words):
            try:
                soup = BeautifulSoup(page_text, 'html.parser')
                date_element = soup.find('meta', property='article:published_time')
                if date_element:
                    date_str = date_element['content'][:10]
                    date = datetime.strptime(date_str, '%Y-%m-%d')
                    if date >= start_date:
                        result.append((url, date_str, char_count))
                else:
                    result.append((url, "작성일자 없음", char_count))
            except Exception as e:
                print(f"오류 발생: {e}")

    return result

def save_results_to_file(results, file_path):
    with open(file_path, 'w', encoding='utf-8') as file:
        for url, date, char_count in results:
            file.write(f"{url} (작성일: {date}, 글자 수: {char_count})\n")

def select_folder():
    root = tk.Tk()
    root.withdraw()
    folder_path = filedialog.askdirectory()
    return folder_path

def get_search_query():
    root = tk.Tk()
    root.withdraw()
    query = simpledialog.askstring("검색어 입력", "검색할 단어를 입력하세요 (공백으로 구분):")
    return query.split()

def get_exclude_words():
    root = tk.Tk()
    root.withdraw()
    exclude_words = simpledialog.askstring("제외할 단어 입력", "제외할 단어를 입력하세요 (공백으로 구분):")
    return exclude_words.split()

def get_min_chars():
    root = tk.Tk()
    root.withdraw()
    min_chars = simpledialog.askinteger("최소 글자 수 입력", "검색할 페이지의 최소 글자 수를 입력하세요:")
    return min_chars

def get_num_results():
    root = tk.Tk()
    root.withdraw()
    num_results = simpledialog.askinteger("검색 결과 개수 입력", "검색 결과로 표시할 웹페이지의 개수를 입력하세요:")
    return num_results

def get_start_date():
    root = tk.Tk()
    root.withdraw()
    start_date_str = simpledialog.askstring("시작 날짜 입력", "검색할 시작 날짜를 입력하세요 (YYYYMMDD):")
    start_date = datetime.strptime(start_date_str, '%Y%m%d')
    return start_date

# 검색어 입력 받기
query = get_search_query()

# 제외할 단어 입력 받기
exclude_words = get_exclude_words()

# 검색할 페이지의 최소 글자 수 입력 받기
min_chars = get_min_chars()

# 검색 결과 개수 입력 받기
num_results = get_num_results()

# 검색 시작 날짜 입력 받기
start_date = get_start_date()

# 지정된 조건에 맞는 페이지 검색
print(f'"{" ".join(query)}" 검색 시작...')

# 비동기 함수 실행
result_pages = asyncio.run(search_pages(query, exclude_words, min_chars, num_results, start_date))
print("검색 완료!")

# 결과 저장 폴더 선택
print("결과를 저장할 폴더를 선택하세요.")
save_path = select_folder()

# 결과 저장 파일 이름 설정
file_name = f"{'_'.join(query)}_results.txt"

# 파일 경로 생성
file_path = os.path.join(save_path, file_name)

# 결과를 파일로 저장
print("검색 결과 파일 저장 중...")
save_results_to_file(result_pages, file_path)
print(f"검색 결과가 {file_path}에 저장되었습니다.")
